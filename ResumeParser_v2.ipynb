{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResumeParser-v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9MLpBYK-Dww",
        "outputId": "6a5ceb94-4d2f-4746-f1c3-b2597986ba3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docx2txt\n",
            "  Downloading docx2txt-0.8.tar.gz (2.8 kB)\n",
            "Building wheels for collected packages: docx2txt\n",
            "  Building wheel for docx2txt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3980 sha256=fd293ff5705ccf87ec952d174ee9f21b4f26f3140b647df7727276f34d1f2dca\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/20/b2/473e3aea9a0c0d3e7b2f7bd81d06d0794fec12752733d1f3a8\n",
            "Successfully built docx2txt\n",
            "Installing collected packages: docx2txt\n",
            "Successfully installed docx2txt-0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install docx2txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx2txt"
      ],
      "metadata": {
        "id": "sm0TG76P-UpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = \"/content/job_description_v2.docx\""
      ],
      "metadata": {
        "id": "KrfD6MnE-o9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = \"/content/VandanaKakkar_Resume_DataScientist_finalversion.docx\"\n",
        "resume2 = \"/content/VandanaKakkar_Resume_BA2.docx\""
      ],
      "metadata": {
        "id": "VO0Gzf8e_ULO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_description = docx2txt.process(\"/content/job_description_v2.docx\")#changing doc file to text format"
      ],
      "metadata": {
        "id": "l9AKXZNL_Y3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resume = docx2txt.process(\"/content/VandanaKakkar_Resume_DataScientist_finalversion.docx\")#changing doc file to text format"
      ],
      "metadata": {
        "id": "7d0-WLrf_lWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (resume)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLQ9Imbf_pf9",
        "outputId": "ff375e73-37d7-4c90-f394-2f624929fa9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vandana Kakkar \n",
            "\n",
            "                                                        vandanakakkar.py@gmail.com\n",
            "\n",
            "                  Dwarka, Delhi, 110075\n",
            "\n",
            "                       9899984084\n",
            "\n",
            "                                                               \n",
            "\n",
            "Languages\n",
            "\n",
            "Python, Spark, Hive, Kafka, R, Tableau, Power BI, SQL, Google Analytics\n",
            "\n",
            "Libraries/Framework\n",
            "\n",
            "Numpy, Pandas, NLP, NLTK, Tensorflow, Keras, CNN, NLU, NLG\n",
            "\n",
            "IDE\n",
            "\n",
            "Google Colab, Jupyter, R studio\n",
            "\n",
            "Domain\n",
            "\n",
            "Healthcare, BFSI\n",
            "\n",
            "\n",
            "\n",
            "Skills: Modeling: Ensembled Learning, NLP, CNN\n",
            "\n",
            "           Data Visualization: Tableau, Power BI, R Shiny, Matplotlib\n",
            "\n",
            "           Research and Analytical Skills: Macro, SQL\n",
            "\n",
            "Total Work Experience: 9 years (2 years in Data Science)\n",
            "\n",
            "ACHIEVEMENTS\n",
            "\n",
            "Achieved Second Position in Hackathon organized by IIT, Roorkee\n",
            "\n",
            "Automated Excel report in Power BI and improved the efficiency of preparing the weekly report, which was going to LOC’s across the globe\n",
            "\n",
            "Work Experience: \n",
            "\n",
            "ORGANIZATION       MASTERCARD – Senior Special Engineer (Dec 2019 – Mar 2020) (Contractual Job)\n",
            "\n",
            "Transferred the SQL codes to Hive environment with usage of Python codes\n",
            "\n",
            "Performed Sentiment Analysis with the help of R tool\n",
            "\n",
            "Performed Analysis on Unsupervised Data with the help of Python tool\n",
            "\n",
            "Researched and Prepared presentations regarding profitability reports, policies, internal operating inefficiencies, and industry trends\n",
            "\n",
            "ORGANIZATION       WNS – Assistant Manager (Jan 2019 till Dec 2019)\n",
            "\n",
            "Managed 4 people \n",
            "\n",
            "Performed Text Analysis, Customer Churn Analysis and Time Series Analysis\n",
            "\n",
            "Usage of Tableau for better presentation of data\n",
            "\n",
            "ORGANIZATION       B.A. Continuum – Team Developer (May 2015 till Mar 2018)\n",
            "\n",
            "To do category research and prepare different models with the help of various paid and unpaid sources\n",
            "\n",
            "To perform NLP activities\n",
            "\n",
            "ORGANIZATION      TC International (Jul 2014 till May 2015)\n",
            "\n",
            "To create dynamic dashboards using parameters, filters, calculated fields and applying level of details functionality on Tableau\n",
            "\n",
            "To work on LOD, Windows Formula and with Tableau calculations\n",
            "\n",
            "ORGANIZATION       Neo Research Pvt. Ltd. (Dec 2011 till Jul 2014) \n",
            "\n",
            "To design questionnaires and prepare proposals for the client\n",
            "\n",
            "To consult the clients \n",
            "\n",
            "To do the pilot study and field survey with the help of field people\n",
            "\n",
            "To analyze the result with the use of SPSS tool\n",
            "\n",
            "ORGANIZATION       Market Pulse (Dec 2010 till Dec 2011) \n",
            "\n",
            "To design questionnaires and prepare proposals for the client\n",
            "\n",
            "To consult the clients\n",
            "\n",
            "To do the pilot study and field survey with the help of field people\n",
            "\n",
            "To formulate synthesized reports on kitchen appliances and electrical appliances\n",
            "\n",
            "\n",
            "\n",
            "ORGANIZATION       Deutsche Bank, Gurgaon (Dec 2008 till Feb 2010) \n",
            "\n",
            "Acquired new clients and maintaining relationship with them so as to analyze their needs and cross sell them other financial products\n",
            "\n",
            "\n",
            "\n",
            "Machine Learning Projects:\n",
            "\n",
            "\n",
            "\n",
            "Object Detection of Tax Papers\n",
            "\n",
            "I have implemented the YOLO detection algorithm and evaluated multiple architectures of the underlying convolutional neural network. \n",
            "\n",
            "I have pretrained the model on a large set of labeled images and then applied transfer learning to my specific task.\n",
            "\n",
            "To increase the data set for my specific task, I have applied data augmentation.\n",
            "\n",
            "\n",
            "\n",
            "Cat Dog Classification using CNN \n",
            "\n",
            "First, I loaded and prepared photos of dogs and cats for modelling. Then, I developed a convolutional neural network for photo classification from scratch and improve model performance.\n",
            "\n",
            "This was followed by developing a model for photo classification using transfer learning.\n",
            "\n",
            "\n",
            "\n",
            "MNIST -Number Classification:\n",
            "\n",
            "The task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively. \n",
            "\n",
            "\n",
            "\n",
            "Sentiment Analysis:\n",
            "\n",
            "I identified sentiment using VADER. Then, I built sentiment classifier using word embeddings.\n",
            "\n",
            "\n",
            "\n",
            " Classification of Customer Churn:\n",
            "\n",
            "First, I removed any columns that is not of any use. Then, I encoded Data Types as appropriate. I examined the distribution of the predicted variable (churned). \n",
            "\n",
            "Then, I split the data into train and test sets. Then, I fit Random Forest models with a range of tree numbers and evaluate the out-of-bag error for each of these models. \n",
            "\n",
            "Then, I plot these OOB errors as a function of the number of trees. Then, I extra randomized trees by using ‘ExtraTreeClassifier’ .Bootstrap for randomizing was set to ‘True’ \n",
            "\n",
            "for this model. Compare the out-of-bag errors for the two different types of models. Then, I selected the model that performs well and I calculated error metrics and\n",
            "\n",
            "Confusion matrix on the test data set. Then, I created different objects using multiple Smoting methods. \n",
            "\n",
            "\n",
            "\n",
            "ACADEMIC PROFILE\n",
            "\n",
            "2021 Post Graduate program of Machine Learning and Data Science from IIT, Roorkee (Certificate Awaited)\n",
            "\n",
            "2020 Deep Learning Certification Course from the Institute, Cloudxlab in collaboration with IIT \n",
            "\n",
            "2009 MBA (Marketing and sales) from Amity University\n",
            "\n",
            "2007 Bachelor of Commerce from Delhi University\n",
            "\n",
            "I hereby declare that the above-mentioned details are true to the best of my knowledge.\n",
            "\n",
            "(Vandana Kakkar)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = [job_description, resume]\n",
        "#because I want to see the match percentage between these two files"
      ],
      "metadata": {
        "id": "ur5PkLNg_7lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "RFqlJrGhAUwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv = CountVectorizer()"
      ],
      "metadata": {
        "id": "_BerxTgzAl9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = cv.fit_transform(content)\n",
        "#we are doing this because it will conver all content into vectors\n",
        "#And vectors only can help in giving us match PERCENTAGE \n",
        "#Percentage is a metric. So, vectors are important"
      ],
      "metadata": {
        "id": "vtR6lJJlAsaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "4vXXfM-VBGON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix = cosine_similarity(matrix)"
      ],
      "metadata": {
        "id": "ioHR3trvBX-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d20O4_wSBdyC",
        "outputId": "adbaf84a-ace9-4475-f3e2-d69f9eca71bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 0.57670187],\n",
              "       [0.57670187, 1.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Resume matches by:'+str(similarity_matrix[1][0]*100)+'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYQ9CtxcBnFR",
        "outputId": "ada379e0-88be-44e4-f5cd-b91b7dde34cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume matches by:57.67018709196774%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Resume 2"
      ],
      "metadata": {
        "id": "bClx_yP_B35u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "content2 = [job_description, resume2]"
      ],
      "metadata": {
        "id": "y6pB0P_GCs2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix2 = cv.fit_transform(content2)"
      ],
      "metadata": {
        "id": "_15cxawyCuo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_matrix2 = cosine_similarity(matrix2)"
      ],
      "metadata": {
        "id": "w8nHhPLxC1p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Resume matches by:'+str(similarity_matrix2[1][0]*100)+'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_BEU_DrC7Mx",
        "outputId": "b927e306-6de6-46e2-a04e-6113893b417c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resume matches by:0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "r6F7YoQODGzV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}